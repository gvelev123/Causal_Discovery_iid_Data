{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "503c8b86",
   "metadata": {},
   "source": [
    "### **1. Simulation of i.i.d. Data with nonlinear, nonidentifiable causal transformations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f87a7068",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-29 00:18:00,444 - C:\\Users\\Georg Velev\\anaconda3\\lib\\site-packages\\castle\\backend\\__init__.py[line:36] - INFO: You can use `os.environ['CASTLE_BACKEND'] = backend` to set the backend(`pytorch` or `mindspore`).\n",
      "2024-11-29 00:18:00,691 - C:\\Users\\Georg Velev\\anaconda3\\lib\\site-packages\\castle\\algorithms\\__init__.py[line:36] - INFO: You are using ``pytorch`` as the backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "#Data simulation imports:\n",
    "import pickle\n",
    "from Data_Simulation_Framework.IID_Data_Generation_Process import lin_func,relu_func, Data_Generation_Process\n",
    "from Results_Visualization.Visualization_Tools import plot_sf_er_graph_NodesEdgesMapping\n",
    "from Causal_Discovery_Models.Causal_Discovery import Causal_Discovery\n",
    "from Src.Utils import python_pickle_to_rds_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6890afb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the data generator instance:\n",
    "nonlinear_pattern='ReLU'\n",
    "nonlinearities=[[(1.0,lin_func)],#linear patterns only\n",
    "                [(0.5,lin_func),(0.5,relu_func)],#equal probability\n",
    "               [(0.3,lin_func),(0.7,relu_func)],#70% probability relu\n",
    "                [(0.1,lin_func),(0.9,relu_func)]]#90% probability relu\n",
    "\n",
    "data_generator=Data_Generation_Process(beta_lower_limit=0.5,\n",
    "                betta_upper_limit_values=[1.0,2.0,3.0,4.0],\n",
    "                cont_noise=1.0,\n",
    "                nr_nodes_values=[10,20,50,100],\n",
    "                edge_desnity_values=[0.2,0.3,0.4],\n",
    "                data_scale_values=['original','standardized'],\n",
    "                num_samples=2500,\n",
    "                nonlinearities=nonlinearities)\n",
    "\n",
    "#First simulate the i.i.d. datasets based on the ER graph model:\n",
    "er_simulation=data_generator.large_scale_simulation(graph_type='ER')\n",
    "\n",
    "#Save large sample size ER-base data:\n",
    "data_generator.save_data(frames_descriptions=er_simulation[0],\n",
    "                         true_causal_matrices=er_simulation[1],true_weighted_causal_matrices=er_simulation[2],\n",
    "                         frames=er_simulation[3],nonlinear_pattern=nonlinear_pattern,\n",
    "                         graph_type='ER',sample_size='Large_Sample_Size',\n",
    "                         save_path='../Data_Simulation_Framework/Simulated_Datasets/')\n",
    "\n",
    "#Save small sample size ER-based data:\n",
    "data_generator.save_data(frames_descriptions=er_simulation[0],\n",
    "                         true_causal_matrices=er_simulation[1],true_weighted_causal_matrices=er_simulation[2],\n",
    "                         frames=er_simulation[3],nonlinear_pattern=nonlinear_pattern,\n",
    "                         graph_type='ER',sample_size='Small_Sample_Size',\n",
    "                         save_path='../Data_Simulation_Framework/Simulated_Datasets/')\n",
    "\n",
    "#Prepare avg number of edges per number of nodes for SF graph type:\n",
    "avg_number_edges=data_generator.get_avg_number_edges_ER_graph(frames_descriptions=er_simulation[0],\n",
    "                                                             save_path_edge_mapping='../Performance_Evaluation_Framework/Results/Avg_Number_Edges.pkl')\n",
    "\n",
    "#Simulate SF-based data:\n",
    "sf_simulation=data_generator.large_scale_simulation(graph_type='SF',avg_number_edges=avg_number_edges)\n",
    "\n",
    "#Save large sample size SF-based data:\n",
    "data_generator.save_data(frames_descriptions=sf_simulation[0],\n",
    "                         true_causal_matrices=sf_simulation[1],true_weighted_causal_matrices=sf_simulation[2],\n",
    "                         frames=sf_simulation[3],nonlinear_pattern=nonlinear_pattern,\n",
    "                         graph_type='SF',sample_size='Large_Sample_Size',\n",
    "                         save_path='../Data_Simulation_Framework/Simulated_Datasets/')\n",
    "\n",
    "#Save small sample size SF-based data:\n",
    "data_generator.save_data(frames_descriptions=sf_simulation[0],\n",
    "                         true_causal_matrices=sf_simulation[1],true_weighted_causal_matrices=sf_simulation[2],\n",
    "                         frames=sf_simulation[3],nonlinear_pattern=nonlinear_pattern,\n",
    "                         graph_type='SF',sample_size='Small_Sample_Size',\n",
    "                         save_path='../Data_Simulation_Framework/Simulated_Datasets/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b376f6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sf_er_graph_NodesEdgesMapping(er_frames_descriptions=er_simulation[0],\n",
    "                                      sf_frames_descriptions=sf_simulation[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40625f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simulate 40 datasets with increased beta upper limit:\n",
    "beta_upper_limit_generator=Data_Generation_Process(beta_lower_limit=0.5,\n",
    "                betta_upper_limit_values=[1.0,4.0,6.0,8.0],\n",
    "                cont_noise=1.0,nr_nodes_values=[20],\n",
    "                edge_desnity_values=[0.3],data_scale_values=['original'],\n",
    "                num_samples=2500,nonlinearities=[[(0.1,lin_func),(0.9,relu_func)]])\n",
    "\n",
    "betas_simulaiton=beta_upper_limit_generator.large_scale_simulation(graph_type='SF',avg_number_edges={'Nodes_20':{0.3:avg_number_edges['Nodes_20'][0.3]}})\n",
    "\n",
    "beta_upper_limit_generator.save_data(frames_descriptions=betas_simulaiton[0],\n",
    "                         true_causal_matrices=betas_simulaiton[1],true_weighted_causal_matrices=betas_simulaiton[2],\n",
    "                         frames=betas_simulaiton[3],nonlinear_pattern=nonlinear_pattern,\n",
    "                         graph_type='SF',sample_size='Large_Sample_Size',\n",
    "                         save_path='../Data_Simulation_Framework/Simulated_Datasets/Betas_')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc9e91c",
   "metadata": {},
   "source": [
    "### **2. Application of Causal Discovery Models on simulated Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6dfcf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Data_Simulation_Framework/Simulated_Datasets/ER_Large_Sample_Size_Datasets_ReLU_10_nodes.pkl','rb') as f:\n",
    "    frames_list=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eef05956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DASK Client Dashboard Link:  http://127.0.0.1:8787/status\n",
      "Using default cache_path: `C:\\Users\\Georg Velev\\Desktop\\Humboldt Uni\\CSL_Benchmark_Study\\Causal_Discovery_iid_Data\\Src\\cache`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 22:12:42,055 - C:\\Users\\Georg Velev\\anaconda3\\lib\\site-packages\\jax\\_src\\xla_bridge.py[line:622] - INFO: Unable to initialize backend 'cuda': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "2024-11-28 22:12:42,057 - C:\\Users\\Georg Velev\\anaconda3\\lib\\site-packages\\jax\\_src\\xla_bridge.py[line:622] - INFO: Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "2024-11-28 22:12:42,064 - C:\\Users\\Georg Velev\\anaconda3\\lib\\site-packages\\jax\\_src\\xla_bridge.py[line:622] - INFO: Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'\n",
      "2024-11-28 22:12:42,065 - C:\\Users\\Georg Velev\\anaconda3\\lib\\site-packages\\jax\\_src\\xla_bridge.py[line:636] - WARNING: No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "causal_discovery=Causal_Discovery(frames_list=frames_list[:3],\n",
    "                                  index_frame_description=0,\n",
    "                                  index_true_adjacency=1,\n",
    "                                  index_true_weighted_adjacency=2,\n",
    "                                  index_frame=3)\n",
    "causal_discovery.extract_causal_graphs()\n",
    "causal_discovery_demo=causal_discovery.causal_discovery_results\n",
    "causal_discovery_demo_rds=[csl_itm.copy() for csl_itm in causal_discovery_demo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8be92a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the the results in RDS format:\n",
    "save_file_path='../Performance_Evaluation_Framework/Results/Causal_Discovery_Results_Demo.rds'\n",
    "python_pickle_to_rds_r(frames_list=causal_discovery_demo_rds,save_path=save_file_path)\n",
    "#Run the script Hybrid_Bayesian_Networks.R in the folder Causal_Discovery_Models in order to\n",
    "#collect the results for the same file from bayesian hybrid networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1863d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Append Results from hybrid bayesian networks computed in R to the python results list:\n",
    "with open('../Performance_Evaluation_Framework/Results/Causal_Discovery_Results_Demo_Hybrid_Bayesian_Networks.pkl','rb') as f:\n",
    "    causal_discovery_demo_hbn=pickle.load(f)\n",
    "\n",
    "for csl_index in range(0,len(causal_discovery_demo)):\n",
    "    #Convert the rds-to-pickle dataframe in python format and compare it to the frames in the python file,\n",
    "    #to make sure that the results from hybrid bayesian networks are mapped to those from the other models:\n",
    "    rds_frame=causal_discovery_demo_hbn[csl_index]['dataset']\n",
    "    cols_array=[int(col) for col in rds_frame.columns]\n",
    "    rds_frame.columns=cols_array\n",
    "    indices=[int(index_value) for index_value in rds_frame.index]\n",
    "    rds_frame.index=indices\n",
    "\n",
    "    if causal_discovery_demo[csl_index][3].equals(rds_frame):\n",
    "        causal_discovery_demo[csl_index][-1]['pctabu']=causal_discovery_demo_hbn[csl_index]['pctabu']\n",
    "        causal_discovery_demo[csl_index][-1]['mmtabu']=causal_discovery_demo_hbn[csl_index]['mmtabu']\n",
    "        causal_discovery_demo[csl_index][-1]['fedtabu']=causal_discovery_demo_hbn[csl_index]['fedtabu']\n",
    "    else:\n",
    "        print('Mismatch found at index: ',csl_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfca7d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Overwrite the frames list with all results in the instance of causal discovery & save it in the result folder:\n",
    "causal_discovery.set_causal_discovery_results(current_csl_results=causal_discovery_demo)\n",
    "causal_discovery.save_csl_results(save_path='../Performance_Evaluation_Framework/Results/Causal_Discovery_Results_Demo.pkl',\n",
    "                                 save_for_evaluation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d8f0ac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(causal_discovery_demo[0][-1].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df6455a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848980fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import avici\n",
    "#from dask.distributed import Client\n",
    "#import pickle\n",
    "#import torch\n",
    "#import numpy as np\n",
    "#import networkx as nx\n",
    "\n",
    "#import math\n",
    "#import torch.nn as nn\n",
    "#import copy\n",
    "#from tqdm.auto import tqdm\n",
    "#import typing\n",
    "\n",
    "#from scipy.stats import ttest_ind\n",
    "#import pandas as pd\n",
    "#sklearn\n",
    "\n",
    "#from abc import ABCMeta, abstractmethod\n",
    "#from pygam import LinearGAM, s\n",
    "#from pygam.terms import Term, TermList\n",
    "\n",
    "#from dataclasses import dataclass, field\n",
    "#from warnings import warn\n",
    "#import inspect\n",
    "#import warnings\n",
    "#from collections import defaultdict\n",
    "#from copy import deepcopy\n",
    "#from copy import copy\n",
    "#from importlib.metadata import version  # type: ignore\n",
    "#import types\n",
    "#from itertools import combinations\n",
    "#from importlib.metadata import version  # type: ignore\n",
    "\n",
    "#import igraph as ig\n",
    "#import matplotlib.pyplot as plt\n",
    "#from cdt.metrics import SID\n",
    "#from rpy2 import robjects\n",
    "#from interpret.glassbox import ExplainableBoostingRegressor\n",
    "\n",
    "#import sys\n",
    "#sys.path.append(\"..\")\n",
    "#import seaborn as sns\n",
    "#import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8833e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import avici,dask, pickle, torch, numpy, networkx, math, copy, tqdm, typing, scipy, pandas, sklearn, abc, pygam, dataclasses, warnings, inspect, collections, copy, importlib, types, itertools, igraph, matplotlib, cdt, rpy2, interpret, seaborn, statsmodels\n",
    "\n",
    "packages=[avici,dask, pickle, torch, numpy, networkx, math, copy, tqdm, typing, scipy, pandas, sklearn, \n",
    "abc, pygam, dataclasses, warnings, inspect, collections, copy, importlib, types, \n",
    "itertools, igraph, matplotlib, cdt, rpy2, interpret, seaborn, statsmodels]\n",
    "\n",
    "for pkg in packages:\n",
    "    try:\n",
    "        print(str(pkg).split(' ')[1].split(\"'\")[1],'==',pkg.__version__)\n",
    "    except:\n",
    "        print('In.built python package.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
